"""
é¢è¯•å®˜æ™ºèƒ½ä½“

å¯¹å¤–æä¾›çš„ç»Ÿä¸€æ¥å£ï¼Œå°è£…æ‰€æœ‰é¢è¯•å®˜ Agent åŠŸèƒ½
åç«¯åªéœ€è¦è°ƒç”¨è¿™ä¸ªæ¨¡å—çš„æ–¹æ³•ï¼Œæ— éœ€å…³å¿ƒå†…éƒ¨å®ç°
"""

import os
import json
import asyncio
import random
from typing import Dict, Any, List, Optional, Tuple, Callable
from datetime import datetime
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage

from .state import (
    InterviewState, 
    InterviewStage, 
    QuestionRecord,
    create_initial_state
)
from .graph import interview_graph
from .context_manager import ContextManager
from .tools.rag_tool import rag_tool
from .tools.web_search_tool import web_search_tool
from .prompts import (
    INTERVIEWER_SYSTEM_PROMPT,
    ANSWER_ANALYSIS_PROMPT,
    FOLLOW_UP_PROMPT,
    STAGE_TRANSITION_PROMPT,
    OPENING_PROMPT,
    CLOSING_PROMPT,
    PREFETCH_PROMPT,
    FILLER_MESSAGES
)

load_dotenv()


class InterviewerAgent:
    """
    é¢è¯•å®˜æ™ºèƒ½ä½“
    
    æä¾›å®Œæ•´çš„é¢è¯•å®˜åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
    - é¢è¯•åˆå§‹åŒ–
    - é—®é¢˜ç”Ÿæˆï¼ˆå¸¦å¼‚æ­¥é¢„å–ï¼‰
    - å›ç­”åˆ†æ
    - é˜¶æ®µç®¡ç†
    - ä¸Šä¸‹æ–‡æŒä¹…åŒ–
    
    ä½¿ç”¨ç¤ºä¾‹:
    ```python
    agent = InterviewerAgent()
    
    # 1. åˆå§‹åŒ–é¢è¯•
    state, opening = await agent.initialize_interview(
        session_id="xxx",
        user_id="user123",
        job_name="Pythonå¼€å‘å·¥ç¨‹å¸ˆ",
        resume_text="..."
    )
    
    # 2. è·å–é—®é¢˜
    result = await agent.get_next_question(state)
    question = result["question"]
    thinking_msg = result.get("thinking_message")  # å¦‚æœéœ€è¦ç­‰å¾…ï¼Œå…ˆæ’­æ”¾è¿™ä¸ª
    
    # 3. å¤„ç†å›ç­”
    analysis = await agent.process_answer(state, user_answer)
    
    # 4. ç»“æŸé¢è¯•
    closing = await agent.end_interview(state)
    ```
    """
    
    def __init__(self):
        """åˆå§‹åŒ–é¢è¯•å®˜ Agent"""
        # LLM å®¢æˆ·ç«¯
        self.llm = ChatOpenAI(
            model="deepseek-chat",
            api_key=os.getenv("Deepseek_API_Key"),
            base_url=os.getenv("DEEPSEEK_BASE_URL"),
            temperature=0.7
        )
        
        self.llm_precise = ChatOpenAI(
            model="deepseek-chat",
            api_key=os.getenv("Deepseek_API_Key"),
            base_url=os.getenv("DEEPSEEK_BASE_URL"),
            temperature=0.3
        )
        
        # é¢„å–ä»»åŠ¡ç¼“å­˜
        self._prefetch_cache: Dict[str, asyncio.Task] = {}
        
        # ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¼“å­˜
        self._context_managers: Dict[str, ContextManager] = {}
        
    # ==================== åˆå§‹åŒ– ====================
    
    async def initialize_interview(
        self,
        session_id: str,
        user_id: str,
        job_name: str,
        resume_text: str
    ) -> Tuple[InterviewState, str]:
        """
        åˆå§‹åŒ–é¢è¯•
        
        Args:
            session_id: ä¼šè¯ID
            user_id: ç”¨æˆ·ID
            job_name: ç›®æ ‡å²—ä½
            resume_text: ç®€å†æ–‡æœ¬
            
        Returns:
            (åˆå§‹çŠ¶æ€, å¼€åœºç™½æ–‡æœ¬)
        """
        print(f"[Interviewer Agent] ğŸš€ Initializing interview for {user_id}")
        
        # 1. åˆ›å»ºä¸Šä¸‹æ–‡ç®¡ç†å™¨
        context_manager = ContextManager(session_id, user_id, job_name)
        context_file = await context_manager.initialize(resume_text)
        self._context_managers[session_id] = context_manager
        
        # 2. åˆå§‹åŒ– RAG å·¥å…·ï¼ˆé¢„åŠ è½½é¢˜åº“ï¼‰
        await rag_tool.initialize()
        
        # 3. åˆ›å»ºåˆå§‹çŠ¶æ€
        state = create_initial_state(
            session_id=session_id,
            user_id=user_id,
            job_name=job_name,
            resume_text=resume_text,
            context_file_path=context_file
        )
        
        # 4. ç”Ÿæˆå¼€åœºç™½
        opening = await self._generate_opening(job_name)
        
        # 5. å¯åŠ¨é¢„å–ä»»åŠ¡ï¼ˆä¸ºç¬¬ä¸€ä¸ªé—®é¢˜é¢„çƒ­ï¼‰
        self._start_prefetch(state)
        
        print(f"[Interviewer Agent] âœ… Interview initialized. Context file: {context_file}")
        
        return state, opening
    
    async def _generate_opening(self, job_name: str, candidate_name: str = "åŒå­¦") -> str:
        """ç”Ÿæˆå¼€åœºç™½"""
        prompt = OPENING_PROMPT.format(
            candidate_name=candidate_name,
            job_name=job_name
        )
        
        try:
            response = await self.llm.ainvoke([
                SystemMessage(content="ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ AI é¢è¯•å®˜ã€‚"),
                HumanMessage(content=prompt)
            ])
            return response.content.strip()
        except Exception as e:
            print(f"[Interviewer Agent] âŒ Opening generation error: {e}")
            return f"ä½ å¥½{candidate_name}ï¼Œæˆ‘æ˜¯ä»Šå¤©çš„ AI é¢è¯•å®˜ã€‚æ¬¢è¿å‚åŠ {job_name}å²—ä½çš„é¢è¯•ã€‚è¯·ç¡®è®¤ä½ çš„è®¾å¤‡å‡†å¤‡å°±ç»ªï¼Œå‡†å¤‡å¥½äº†å°±å¯ä»¥å¼€å§‹ã€‚"
    
    # ==================== é—®é¢˜ç”Ÿæˆ ====================
    
    async def get_next_question(
        self,
        state: InterviewState,
        on_thinking: Optional[Callable[[str], None]] = None
    ) -> Dict[str, Any]:
        """
        è·å–ä¸‹ä¸€ä¸ªé¢è¯•é—®é¢˜
        
        Args:
            state: å½“å‰é¢è¯•çŠ¶æ€
            on_thinking: æ€è€ƒæ¶ˆæ¯å›è°ƒï¼ˆç”¨äºå‘é€ filler wordsï¼‰
            
        Returns:
            {
                "question": str,
                "reference_answer": str,
                "thinking_message": Optional[str],  # å¦‚æœæœ‰å»¶è¿Ÿéœ€è¦æ’­æ”¾
                "stage": str,
                "is_stage_changed": bool
            }
        """
        session_id = state['session_id']
        current_stage = state['current_stage']
        
        # æ¸…é™¤æ—§çš„é¢„å–ç¼“å­˜ï¼ˆç¦ç”¨é¢„å–ä»¥é¿å…é˜¶æ®µé”™ä¹±ï¼‰
        if session_id in self._prefetch_cache:
            old_task = self._prefetch_cache[session_id]
            if not old_task.done():
                old_task.cancel()
            del self._prefetch_cache[session_id]
        
        print(f"[Interviewer Agent] ğŸ¯ Generating question for stage: {current_stage}")
        
        # å‘é€æ€è€ƒæ¶ˆæ¯ï¼ˆå¦‚æœæœ‰å›è°ƒï¼‰
        thinking_msg = random.choice(FILLER_MESSAGES["thinking"])
        if on_thinking:
            on_thinking(thinking_msg)
        
        # ä½¿ç”¨ LangGraph ç”Ÿæˆé—®é¢˜
        result = await interview_graph.generate_question(state)
        
        # æ›´æ–°çŠ¶æ€
        state['current_question'] = result['question']
        state['stage_question_count'] += 1
        
        # è®°å½•åˆ°ä¸Šä¸‹æ–‡æ–‡ä»¶
        context_manager = self._context_managers.get(session_id)
        if context_manager:
            await context_manager.append_question(
                question=result['question'],
                stage=state['current_stage'],
                question_index=len(state['question_history']) + 1,
                source=result.get('source', 'generated')
            )
        
        print(f"[Interviewer Agent] âœ… Question generated for {current_stage}: {result['question'][:50]}...")
        
        return {
            "question": result['question'],
            "reference_answer": result.get('reference_answer', ''),
            "thinking_message": result.get('thinking_message'),
            "stage": state['current_stage'],
            "is_stage_changed": False
        }
    
    def _start_prefetch(self, state: InterviewState):
        """[DISABLED] é¢„å–åŠŸèƒ½å·²ç¦ç”¨ä»¥é¿å…é˜¶æ®µé”™ä¹±"""
        # ç¦ç”¨é¢„å–ï¼Œå› ä¸ºå®ƒä¼šå¯¼è‡´é˜¶æ®µä¸åŒæ­¥
        pass
    
    # ==================== å›ç­”å¤„ç† ====================
    
    async def process_answer(
        self,
        state: InterviewState,
        user_answer: str
    ) -> Dict[str, Any]:
        """
        å¤„ç†ç”¨æˆ·å›ç­”
        
        Args:
            state: å½“å‰é¢è¯•çŠ¶æ€
            user_answer: ç”¨æˆ·çš„å›ç­”
            
        Returns:
            {
                "score": float,
                "feedback": str,
                "action": str,  # "follow_up", "next_question", "next_stage", "end_interview"
                "follow_up_question": Optional[str],
                "should_advance_stage": bool,
                "next_stage": Optional[str]
            }
        """
        print(f"[Interviewer Agent] ğŸ“ Processing answer: {user_answer[:50]}...")
        
        session_id = state['session_id']
        current_question = state.get('current_question', '')
        
        # 1. åˆ†æå›ç­”
        analysis = await self._analyze_answer(state, user_answer)
        
        # 2. è®°å½•åˆ°å†å²
        record = QuestionRecord(
            question=current_question,
            answer=user_answer,
            score=analysis.get('score', 5),
            feedback=analysis.get('feedback', ''),
            stage=state['current_stage'],
            is_follow_up=state['follow_up_count'] > 0,
            reference_answer=state.get('output_reference'),
            source='agent',
            timestamp=datetime.now().isoformat()
        )
        state['question_history'].append(record)
        
        # 3. æ›´æ–°åˆ†æ•°
        state['total_score'] += analysis.get('score', 5)
        
        # æ›´æ–°é˜¶æ®µåˆ†æ•°
        stage_key = state['current_stage']
        if stage_key not in state['stage_scores']:
            state['stage_scores'][stage_key] = 0
        state['stage_scores'][stage_key] += analysis.get('score', 5)
        
        # 4. è®°å½•åˆ°ä¸Šä¸‹æ–‡æ–‡ä»¶
        context_manager = self._context_managers.get(session_id)
        if context_manager:
            await context_manager.append_answer(
                answer=user_answer,
                score=analysis.get('score'),
                feedback=analysis.get('feedback')
            )
        
        # 5. æ ¹æ®åˆ†æç»“æœå†³å®šä¸‹ä¸€æ­¥
        action = analysis.get('action', 'next_question')
        should_advance = analysis.get('should_advance_stage', False)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿›å…¥ä¸‹ä¸€é˜¶æ®µ
        if should_advance or action == 'next_stage':
            next_stage = InterviewStage.get_next_stage(state['current_stage'])
            
            # å…³é”®æ ¡éªŒï¼šç¡®ä¿åªèƒ½å‘å‰æ¨è¿›ï¼Œä¸èƒ½è·³å›
            if next_stage:
                stage_order = InterviewStage.get_stage_order()
                current_idx = stage_order.index(state['current_stage'])
                next_idx = stage_order.index(next_stage)
                
                # åªå…è®¸å‘å‰è¿›å…¥ä¸‹ä¸€ä¸ªé˜¶æ®µ
                if next_idx == current_idx + 1:
                    # è®°å½•é˜¶æ®µè½¬æ¢
                    if context_manager:
                        await context_manager.append_stage_transition(
                            from_stage=state['current_stage'],
                            to_stage=next_stage
                        )
                    
                    print(f"[Interviewer Agent] ğŸš¦ Stage transition: {state['current_stage']} -> {next_stage}")
                    state['current_stage'] = next_stage
                    state['stage_question_count'] = 0
                    state['follow_up_count'] = 0
                    state['stage_start_time'] = datetime.now().isoformat()
                    
                    analysis['next_stage'] = next_stage
                    analysis['action'] = 'next_stage'
                else:
                    # é˜²æ­¢è·³è·ƒæˆ–åé€€
                    print(f"[Interviewer Agent] âš ï¸ Prevented invalid stage jump: {state['current_stage']} -> {next_stage}")
                    analysis['should_advance_stage'] = False
                    analysis['action'] = 'next_question'
            else:
                # æ²¡æœ‰ä¸‹ä¸€é˜¶æ®µï¼Œç»“æŸé¢è¯•
                analysis['action'] = 'end_interview'
        
        # å¤„ç†è¿½é—®
        if action == 'follow_up':
            state['follow_up_count'] += 1
            
            # å¦‚æœè¿½é—®è¶…è¿‡é™åˆ¶ï¼Œæ”¹ä¸ºä¸‹ä¸€é¢˜
            if state['follow_up_count'] >= 2:
                analysis['action'] = 'next_question'
                state['follow_up_count'] = 0
        else:
            state['follow_up_count'] = 0
        
        # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾ç»“æŸé˜¶æ®µ
        if state['current_stage'] == InterviewStage.CLOSING:
            analysis['action'] = 'end_interview'
        
        return analysis
    
    async def _analyze_answer(
        self,
        state: InterviewState,
        user_answer: str
    ) -> Dict[str, Any]:
        """åˆ†æç”¨æˆ·å›ç­”"""
        stage_config = InterviewStage.get_stage_config(state['current_stage'])
        
        # è·å–é˜¶æ®µé¡ºåºä¿¡æ¯
        stage_order = InterviewStage.get_stage_order()
        current_idx = stage_order.index(state['current_stage'])
        next_stage = stage_order[current_idx + 1] if current_idx < len(stage_order) - 1 else "END"
        
        # æ„å»º promptï¼ŒåŒ…å«é˜¶æ®µé¡ºåºçº¦æŸ
        prompt = ANSWER_ANALYSIS_PROMPT.format(
            current_question=state.get('current_question', ''),
            reference_answer=state.get('output_reference', 'æ— '),
            user_answer=user_answer,
            current_stage=state['current_stage'],
            stage_question_count=state['stage_question_count'],
            follow_up_count=state['follow_up_count'],
            resume_summary=state['resume_text'][:1000]
        )
        
        # æ·»åŠ é¢å¤–çš„é˜¶æ®µçº¦æŸ
        stage_constraint = f"""

æ³¨æ„ï¼šå½“å‰é˜¶æ®µæ˜¯ [{state['current_stage']}]ï¼Œä¸‹ä¸€ä¸ªé˜¶æ®µåªèƒ½æ˜¯ [{next_stage}]ã€‚
é˜¶æ®µé¡ºåºï¼šopening â†’ self_intro â†’ project_deep_dive â†’ basic_knowledge â†’ scenario_algorithm â†’ reverse_interview â†’ closing
ç»å¯¹ç¦æ­¢è·³å›å·²å®Œæˆçš„é˜¶æ®µï¼"""
        
        try:
            response = await self.llm_precise.ainvoke([
                SystemMessage(content=INTERVIEWER_SYSTEM_PROMPT),
                HumanMessage(content=prompt + stage_constraint)
            ])
            
            content = response.content.strip()
            
            # æå– JSON
            if '{' in content:
                start = content.index('{')
                end = content.rindex('}') + 1
                result = json.loads(content[start:end])
            else:
                result = {
                    "score": 5,
                    "feedback": "å›ç­”å·²è®°å½•",
                    "action": "next_question",
                    "should_advance_stage": False
                }
            
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥è¿›å…¥ä¸‹ä¸€é˜¶æ®µ
            min_questions = stage_config.get('min_questions', 1)
            max_questions = stage_config.get('max_questions', 3)
            
            if state['stage_question_count'] >= max_questions:
                result['should_advance_stage'] = True
            elif state['stage_question_count'] >= min_questions:
                # å¦‚æœå›ç­”è´¨é‡ç¨³å®šï¼Œå¯ä»¥æå‰è¿›å…¥ä¸‹ä¸€é˜¶æ®µ
                if result.get('score', 5) >= 7:
                    result['should_advance_stage'] = True
            
            print(f"[Interviewer Agent] âœ… Analysis: score={result.get('score')}, action={result.get('action')}")
            return result
            
        except Exception as e:
            print(f"[Interviewer Agent] âŒ Analysis error: {e}")
            return {
                "score": 5,
                "feedback": "ç³»ç»Ÿå¤„ç†ä¸­",
                "action": "next_question",
                "should_advance_stage": False
            }
    
    # ==================== é˜¶æ®µç®¡ç† ====================
    
    async def get_stage_info(self, state: InterviewState) -> Dict[str, Any]:
        """
        è·å–å½“å‰é˜¶æ®µä¿¡æ¯
        
        Args:
            state: å½“å‰é¢è¯•çŠ¶æ€
            
        Returns:
            é˜¶æ®µé…ç½®ä¿¡æ¯
        """
        stage_config = InterviewStage.get_stage_config(state['current_stage'])
        return {
            "current_stage": state['current_stage'],
            "stage_name": stage_config.get('name', ''),
            "stage_description": stage_config.get('description', ''),
            "questions_asked": state['stage_question_count'],
            "min_questions": stage_config.get('min_questions', 1),
            "max_questions": stage_config.get('max_questions', 3),
            "all_stages": [s.value for s in InterviewStage.get_stage_order()]
        }
    
    async def force_next_stage(self, state: InterviewState) -> Optional[InterviewStage]:
        """
        å¼ºåˆ¶è¿›å…¥ä¸‹ä¸€é˜¶æ®µ
        
        Args:
            state: å½“å‰é¢è¯•çŠ¶æ€
            
        Returns:
            æ–°çš„é˜¶æ®µï¼Œå¦‚æœå·²æ˜¯æœ€åé˜¶æ®µåˆ™è¿”å› None
        """
        next_stage = InterviewStage.get_next_stage(state['current_stage'])
        
        if next_stage:
            context_manager = self._context_managers.get(state['session_id'])
            if context_manager:
                await context_manager.append_stage_transition(
                    from_stage=state['current_stage'],
                    to_stage=next_stage
                )
            
            state['current_stage'] = next_stage
            state['stage_question_count'] = 0
            state['follow_up_count'] = 0
            state['stage_start_time'] = datetime.now().isoformat()
            
            print(f"[Interviewer Agent] â¡ï¸ Forced stage transition to: {next_stage}")
        
        return next_stage
    
    # ==================== ç»“æŸé¢è¯• ====================
    
    async def end_interview(self, state: InterviewState) -> Dict[str, Any]:
        """
        ç»“æŸé¢è¯•
        
        Args:
            state: å½“å‰é¢è¯•çŠ¶æ€
            
        Returns:
            {
                "closing_text": str,
                "summary": {
                    "total_questions": int,
                    "average_score": float,
                    "duration_minutes": int,
                    "stage_scores": dict
                }
            }
        """
        session_id = state['session_id']
        print(f"[Interviewer Agent] ğŸ Ending interview: {session_id}")
        
        # 1. è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        total_questions = len(state['question_history'])
        average_score = state['total_score'] / max(total_questions, 1)
        
        start_time = datetime.fromisoformat(state['start_time'])
        duration_minutes = (datetime.now() - start_time).seconds // 60
        
        # 2. ç”Ÿæˆç»“æŸè¯­
        closing_text = await self._generate_closing(
            state['question_history'],
            average_score,
            duration_minutes
        )
        
        # 3. è®°å½•æ€»ç»“åˆ°ä¸Šä¸‹æ–‡æ–‡ä»¶
        context_manager = self._context_managers.get(session_id)
        if context_manager:
            await context_manager.append_summary(
                total_questions=total_questions,
                total_score=state['total_score'],
                stage_scores=state['stage_scores'],
                duration_minutes=duration_minutes
            )
        
        # 4. æ¸…ç†èµ„æº
        await self._cleanup(session_id)
        
        return {
            "closing_text": closing_text,
            "summary": {
                "total_questions": total_questions,
                "average_score": round(average_score, 1),
                "duration_minutes": duration_minutes,
                "stage_scores": state['stage_scores']
            }
        }
    
    async def _generate_closing(
        self,
        question_history: List[Dict],
        overall_score: float,
        duration_minutes: int
    ) -> str:
        """ç”Ÿæˆç»“æŸè¯­"""
        # ç®€åŒ–é—®ç­”å†å²
        qa_summary = "\n".join([
            f"Q: {q.get('question', '')[:50]}... A: è¯„åˆ†{q.get('score', 'N/A')}"
            for q in question_history[-5:]
        ])
        
        prompt = CLOSING_PROMPT.format(
            qa_summary=qa_summary,
            overall_score=overall_score,
            duration_minutes=duration_minutes
        )
        
        try:
            response = await self.llm.ainvoke([
                SystemMessage(content="ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ AI é¢è¯•å®˜ã€‚"),
                HumanMessage(content=prompt)
            ])
            return response.content.strip()
        except Exception as e:
            print(f"[Interviewer Agent] âŒ Closing generation error: {e}")
            return "å¥½çš„ï¼Œä»Šå¤©çš„é¢è¯•å°±åˆ°è¿™é‡Œã€‚æ„Ÿè°¢ä½ çš„å‚ä¸ï¼Œåç»­ç»“æœæˆ‘ä»¬ä¼šé€šè¿‡é‚®ä»¶é€šçŸ¥ä½ ã€‚ç¥ä½ ä¸€åˆ‡é¡ºåˆ©ï¼"
    
    async def _cleanup(self, session_id: str):
        """æ¸…ç†ä¼šè¯èµ„æº"""
        # å–æ¶ˆé¢„å–ä»»åŠ¡
        if session_id in self._prefetch_cache:
            task = self._prefetch_cache[session_id]
            if not task.done():
                task.cancel()
            del self._prefetch_cache[session_id]
        
        # æ¸…ç†ä¸Šä¸‹æ–‡ç®¡ç†å™¨å¼•ç”¨ï¼ˆä¿ç•™æ–‡ä»¶ï¼‰
        if session_id in self._context_managers:
            del self._context_managers[session_id]
        
        print(f"[Interviewer Agent] ğŸ§¹ Cleaned up session: {session_id}")
    
    # ==================== å·¥å…·æ–¹æ³• ====================
    
    async def get_filler_message(self, message_type: str = "thinking") -> str:
        """
        è·å–æ€è€ƒå ä½ç¬¦æ¶ˆæ¯
        
        Args:
            message_type: æ¶ˆæ¯ç±»å‹ (searching, web_search, thinking, transitioning)
            
        Returns:
            éšæœºé€‰æ‹©çš„å ä½ç¬¦æ¶ˆæ¯
        """
        messages = FILLER_MESSAGES.get(message_type, FILLER_MESSAGES["thinking"])
        return random.choice(messages)
    
    async def search_question_bank(
        self,
        keywords: List[str],
        top_k: int = 5
    ) -> List[Dict[str, Any]]:
        """
        ç›´æ¥æœç´¢é¢˜åº“ï¼ˆè°ƒè¯•ç”¨ï¼‰
        
        Args:
            keywords: å…³é”®è¯åˆ—è¡¨
            top_k: è¿”å›æ•°é‡
            
        Returns:
            æ£€ç´¢åˆ°çš„é¢˜ç›®åˆ—è¡¨
        """
        await rag_tool.initialize()
        return await rag_tool.search_by_keywords(keywords, top_k)


# å•ä¾‹å®ä¾‹
interviewer_agent = InterviewerAgent()
