"""
LangGraph é¢è¯•å·¥ä½œæµ

å®šä¹‰é¢è¯•å®˜ Agent çš„å·¥ä½œæµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
- çŠ¶æ€ç®¡ç†
- èŠ‚ç‚¹å®šä¹‰
- è¾¹ç¼˜æ¡ä»¶
- å·¥ä½œæµç¼–æ’
"""

import os
import json
import asyncio
from typing import Dict, Any, List, Optional, Literal
from datetime import datetime
from dotenv import load_dotenv

# LangGraph ç›¸å…³å¯¼å…¥
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage

# æœ¬åœ°æ¨¡å—å¯¼å…¥
from .state import InterviewState, InterviewStage, QuestionRecord, create_initial_state
from .tools.rag_tool import rag_tool
from .tools.web_search_tool import web_search_tool
from .context_manager import ContextManager
from .prompts import (
    INTERVIEWER_SYSTEM_PROMPT,
    KEYWORD_GENERATION_PROMPT,
    QUESTION_GENERATION_PROMPT,
    ANSWER_ANALYSIS_PROMPT,
    FOLLOW_UP_PROMPT,
    STAGE_TRANSITION_PROMPT,
    OPENING_PROMPT,
    CLOSING_PROMPT,
    PREFETCH_PROMPT,
    FILLER_MESSAGES
)

load_dotenv()


class InterviewGraph:
    """
    é¢è¯•å·¥ä½œæµå›¾
    
    ä½¿ç”¨ LangGraph å®ç°é¢è¯•å®˜çš„å†³ç­–æµç¨‹
    """
    
    def __init__(self):
        """åˆå§‹åŒ–é¢è¯•å·¥ä½œæµ"""
        # åˆå§‹åŒ– LLMï¼ˆä½¿ç”¨ DeepSeekï¼‰
        self.llm = ChatOpenAI(
            model="deepseek-chat",
            api_key=os.getenv("Deepseek_API_Key"),
            base_url=os.getenv("DEEPSEEK_BASE_URL"),
            temperature=0.7
        )
        
        self.llm_precise = ChatOpenAI(
            model="deepseek-chat",
            api_key=os.getenv("Deepseek_API_Key"),
            base_url=os.getenv("DEEPSEEK_BASE_URL"),
            temperature=0.3  # æ›´ä½æ¸©åº¦ç”¨äºç²¾ç¡®ä»»åŠ¡
        )
        
        # æ„å»ºå·¥ä½œæµå›¾
        self.graph = self._build_graph()
        self.compiled_graph = self.graph.compile()
        
    def _build_graph(self) -> StateGraph:
        """
        æ„å»º LangGraph å·¥ä½œæµ
        
        èŠ‚ç‚¹ï¼š
        1. generate_keywords - ç”Ÿæˆæœç´¢å…³é”®è¯
        2. rag_search - RAG é¢˜åº“æ£€ç´¢
        3. decide_web_search - å†³å®šæ˜¯å¦éœ€è¦ç½‘ç»œæœç´¢
        4. web_search - ç½‘ç»œæœç´¢ï¼ˆå¯é€‰ï¼‰
        5. generate_question - ç”Ÿæˆé—®é¢˜
        6. output_question - è¾“å‡ºé—®é¢˜ï¼ˆæœ€ç»ˆèŠ‚ç‚¹ï¼‰
        
        Returns:
            StateGraph å®ä¾‹
        """
        # åˆ›å»ºçŠ¶æ€å›¾
        graph = StateGraph(InterviewState)
        
        # æ·»åŠ èŠ‚ç‚¹
        graph.add_node("generate_keywords", self._node_generate_keywords)
        graph.add_node("rag_search", self._node_rag_search)
        graph.add_node("decide_web_search", self._node_decide_web_search)
        graph.add_node("web_search", self._node_web_search)
        graph.add_node("generate_question", self._node_generate_question)
        graph.add_node("output_question", self._node_output_question)
        
        # è®¾ç½®å…¥å£ç‚¹
        graph.set_entry_point("generate_keywords")
        
        # æ·»åŠ è¾¹
        graph.add_edge("generate_keywords", "rag_search")
        graph.add_edge("rag_search", "decide_web_search")
        
        # æ¡ä»¶è¾¹ï¼šå†³å®šæ˜¯å¦è¿›è¡Œç½‘ç»œæœç´¢
        graph.add_conditional_edges(
            "decide_web_search",
            self._should_web_search,
            {
                "search": "web_search",
                "skip": "generate_question"
            }
        )
        
        graph.add_edge("web_search", "generate_question")
        graph.add_edge("generate_question", "output_question")
        graph.add_edge("output_question", END)
        
        return graph
    
    # ==================== èŠ‚ç‚¹å®ç° ====================
    
    async def _node_generate_keywords(self, state: InterviewState) -> Dict[str, Any]:
        """
        èŠ‚ç‚¹ï¼šç”Ÿæˆæœç´¢å…³é”®è¯
        
        æ ¹æ®ç®€å†ã€å²—ä½ã€é˜¶æ®µå’Œä¸Šä¸‹æ–‡ç”Ÿæˆæ£€ç´¢å…³é”®è¯
        """
        print(f"[Graph Node] ğŸ”‘ Generating keywords for stage: {state['current_stage']}")
        
        # æ„å»º Prompt
        prompt = KEYWORD_GENERATION_PROMPT.format(
            resume_summary=state['resume_text'][:1500],
            job_name=state['job_name'],
            current_stage=state['current_stage'],
            recent_context=self._format_recent_qa(state['question_history'][-3:])
        )
        
        try:
            response = await self.llm_precise.ainvoke([
                SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªå…³é”®è¯ç”ŸæˆåŠ©æ‰‹ï¼Œåªè¾“å‡º JSON æ•°ç»„ã€‚"),
                HumanMessage(content=prompt)
            ])
            
            # è§£æå…³é”®è¯
            content = response.content.strip()
            # å°è¯•æå– JSON æ•°ç»„
            if '[' in content and ']' in content:
                start = content.index('[')
                end = content.rindex(']') + 1
                keywords = json.loads(content[start:end])
            else:
                keywords = content.split(',')
            
            keywords = [k.strip().strip('"\'') for k in keywords if k.strip()]
            print(f"[Graph Node] âœ… Generated keywords: {keywords}")
            
            return {"search_keywords": keywords}
            
        except Exception as e:
            print(f"[Graph Node] âŒ Keyword generation error: {e}")
            # åå¤‡å…³é”®è¯
            fallback_keywords = [state['job_name'], state['current_stage']]
            return {"search_keywords": fallback_keywords}
    
    async def _node_rag_search(self, state: InterviewState) -> Dict[str, Any]:
        """
        èŠ‚ç‚¹ï¼šRAG é¢˜åº“æ£€ç´¢
        
        ä½¿ç”¨ç”Ÿæˆçš„å…³é”®è¯ä»é¢˜åº“ä¸­æ£€ç´¢ç›¸å…³é¢˜ç›®
        """
        print(f"[Graph Node] ğŸ“š RAG searching with keywords: {state['search_keywords']}")
        
        # ç¡®ä¿ RAG å·¥å…·å·²åˆå§‹åŒ–
        await rag_tool.initialize()
        
        # æ‰§è¡Œæ£€ç´¢
        results = await rag_tool.search_by_keywords(
            keywords=state['search_keywords'],
            top_k=5
        )
        
        # è¿‡æ»¤å·²é—®è¿‡çš„é¢˜ç›®
        asked_questions = {q['question'] for q in state['question_history']}
        filtered_results = [
            r for r in results 
            if r['question'] not in asked_questions
        ]
        
        print(f"[Graph Node] âœ… RAG returned {len(filtered_results)} unique results")
        
        return {"rag_results": filtered_results}
    
    async def _node_decide_web_search(self, state: InterviewState) -> Dict[str, Any]:
        """
        èŠ‚ç‚¹ï¼šå†³å®šæ˜¯å¦éœ€è¦ç½‘ç»œæœç´¢
        
        æ ¹æ® RAG ç»“æœè´¨é‡å’Œé˜¶æ®µéœ€æ±‚å†³å®š
        """
        rag_results = state.get('rag_results', [])
        current_stage = state['current_stage']
        
        # å†³ç­–é€»è¾‘ï¼š
        # 1. å¦‚æœ RAG ç»“æœä¸ºç©ºæˆ–åˆ†æ•°è¿‡ä½ï¼Œè€ƒè™‘ç½‘ç»œæœç´¢
        # 2. å¦‚æœæ˜¯é¡¹ç›®æ·±æŒ–é˜¶æ®µä¸”ç®€å†æåˆ°ç‰¹å®šæŠ€æœ¯ï¼Œå¯èƒ½éœ€è¦æœç´¢
        # 3. å¦‚æœæ˜¯åœºæ™¯é¢˜é˜¶æ®µï¼Œå¯èƒ½éœ€è¦æœ€æ–°æ¡ˆä¾‹
        
        needs_search = False
        thinking_message = None
        
        if not rag_results:
            needs_search = True
            thinking_message = FILLER_MESSAGES["searching"][0]
        elif all(r.get('score', 0) < 0.5 for r in rag_results):
            needs_search = True
            thinking_message = FILLER_MESSAGES["web_search"][0]
        elif current_stage == InterviewStage.PROJECT_DEEP_DIVE:
            # æ£€æŸ¥ç®€å†ä¸­æ˜¯å¦æœ‰éœ€è¦éªŒè¯çš„æŠ€æœ¯
            resume_lower = state['resume_text'].lower()
            tech_keywords = ['kubernetes', 'kafka', 'elasticsearch', 'tensorflow', 'pytorch']
            if any(tech in resume_lower for tech in tech_keywords):
                needs_search = True
                thinking_message = FILLER_MESSAGES["web_search"][1]
        
        print(f"[Graph Node] ğŸ¤” Web search decision: {needs_search}")
        
        return {
            "needs_web_search": needs_search,
            "thinking_message": thinking_message
        }
    
    def _should_web_search(self, state: InterviewState) -> Literal["search", "skip"]:
        """æ¡ä»¶å‡½æ•°ï¼šåˆ¤æ–­æ˜¯å¦æ‰§è¡Œç½‘ç»œæœç´¢"""
        if state.get('needs_web_search', False):
            return "search"
        return "skip"
    
    async def _node_web_search(self, state: InterviewState) -> Dict[str, Any]:
        """
        èŠ‚ç‚¹ï¼šç½‘ç»œæœç´¢
        
        æ‰§è¡Œç½‘ç»œæœç´¢è·å–è¡¥å……ä¿¡æ¯
        """
        print(f"[Graph Node] ğŸŒ Performing web search...")
        
        # æ„å»ºæœç´¢æŸ¥è¯¢
        keywords = state.get('search_keywords', [])
        job_name = state['job_name']
        query = f"{job_name} {' '.join(keywords)} é¢è¯•é¢˜"
        
        try:
            results = await web_search_tool.search(query, max_results=3)
            print(f"[Graph Node] âœ… Web search returned {len(results)} results")
            return {"web_search_results": results}
        except Exception as e:
            print(f"[Graph Node] âŒ Web search error: {e}")
            return {"web_search_results": []}
    
    async def _node_generate_question(self, state: InterviewState) -> Dict[str, Any]:
        """
        èŠ‚ç‚¹ï¼šç”Ÿæˆé¢è¯•é—®é¢˜
        
        ç»¼åˆæ‰€æœ‰ä¿¡æ¯ç”Ÿæˆæœ€ç»ˆçš„é¢è¯•é—®é¢˜
        """
        current_stage = state['current_stage']
        print(f"[Graph Node] ğŸ’¬ Generating question for stage: {current_stage}")
        
        # è·å–é˜¶æ®µé…ç½®
        stage_config = InterviewStage.get_stage_config(current_stage)
        
        # è·å–é˜¶æ®µé¡ºåºå’Œå½“å‰ä½ç½®
        stage_order = InterviewStage.get_stage_order()
        current_stage_idx = stage_order.index(current_stage) if current_stage in stage_order else 0
        stage_progress = f"({current_stage_idx + 1}/{len(stage_order)})"
        
        # æ ¼å¼åŒ– RAG ç»“æœ
        rag_formatted = self._format_rag_results(state.get('rag_results', []))
        
        # æ ¼å¼åŒ– Web ç»“æœ
        web_formatted = web_search_tool.format_results_for_prompt(
            state.get('web_search_results', [])
        ) if state.get('web_search_results') else "æ— "
        
        # æ„å»º Prompt - å¼ºè°ƒå½“å‰é˜¶æ®µçº¦æŸ
        prompt = QUESTION_GENERATION_PROMPT.format(
            current_stage=f"{current_stage} {stage_progress}",
            stage_description=stage_config.get('description', ''),
            resume_summary=state['resume_text'][:1500],
            job_name=state['job_name'],
            asked_questions=self._format_asked_questions(state['question_history']),
            rag_results=rag_formatted,
            web_results=web_formatted,
            recent_context=self._format_recent_qa(state['question_history'][-3:])
        )
        
        # æ·»åŠ é˜¶æ®µçº¦æŸæç¤º
        stage_constraint = f"""

é‡è¦çº¦æŸï¼š
1. å½“å‰é˜¶æ®µæ˜¯ [{current_stage}]ï¼Œå¿…é¡»ç”Ÿæˆä¸æ­¤é˜¶æ®µåŒ¹é…çš„é—®é¢˜
2. ç»å¯¹ç¦æ­¢ç”Ÿæˆâ€œè¯·åšä¸€ä¸ªè‡ªæˆ‘ä»‹ç»â€æˆ–å¼€åœºç™½ç±»é—®é¢˜ï¼ˆé™¤éå½“å‰é˜¶æ®µæ˜¯ self_introï¼‰
3. å¦‚æœå½“å‰æ˜¯ project_deep_dive é˜¶æ®µï¼Œåº”è¯¢é—®é¡¹ç›®æŠ€æœ¯ç»†èŠ‚
4. å¦‚æœå½“å‰æ˜¯ basic_knowledge é˜¶æ®µï¼Œåº”è¯¢é—®ä¸“ä¸šåŸºç¡€çŸ¥è¯†
5. å¦‚æœå½“å‰æ˜¯ scenario_algorithm é˜¶æ®µï¼Œåº”è¯¢é—®åœºæ™¯æˆ–ç®—æ³•é¢˜
6. é—®é¢˜å¿…é¡»ä¸å·²é—®é—®é¢˜ä¸é‡å¤
"""
        
        try:
            response = await self.llm.ainvoke([
                SystemMessage(content=INTERVIEWER_SYSTEM_PROMPT),
                HumanMessage(content=prompt + stage_constraint)
            ])
            
            # è§£æ JSON å“åº”
            content = response.content.strip()
            # æå– JSON
            if '{' in content:
                start = content.index('{')
                end = content.rindex('}') + 1
                result = json.loads(content[start:end])
            else:
                result = {
                    "question": content,
                    "reference_answer": "",
                    "source": "generated",
                    "difficulty": 3
                }
            
            print(f"[Graph Node] âœ… Generated question: {result.get('question', '')[:50]}...")
            
            return {
                "output_question": result.get('question', ''),
                "output_reference": result.get('reference_answer', '')
            }
            
        except Exception as e:
            print(f"[Graph Node] âŒ Question generation error: {e}")
            # ä½¿ç”¨ RAG ç»“æœä½œä¸ºåå¤‡
            if state.get('rag_results'):
                fallback = state['rag_results'][0]
                return {
                    "output_question": fallback['question'],
                    "output_reference": fallback.get('reference_answer', '')
                }
            return {
                "output_question": "è¯·ä»‹ç»ä¸€ä¸‹ä½ æœ€è¿‘å‚ä¸çš„ä¸€ä¸ªé¡¹ç›®ã€‚",
                "output_reference": ""
            }
    
    async def _node_output_question(self, state: InterviewState) -> Dict[str, Any]:
        """
        èŠ‚ç‚¹ï¼šè¾“å‡ºé—®é¢˜ï¼ˆæœ€ç»ˆèŠ‚ç‚¹ï¼‰
        
        å‡†å¤‡æœ€ç»ˆè¾“å‡º
        """
        print(f"[Graph Node] ğŸ“¤ Outputting question: {state.get('output_question', '')[:50]}...")
        return {}  # çŠ¶æ€å·²ç»åŒ…å«è¾“å‡º
    
    # ==================== è¾…åŠ©æ–¹æ³• ====================
    
    def _format_recent_qa(self, qa_list: List[Dict]) -> str:
        """æ ¼å¼åŒ–æœ€è¿‘çš„é—®ç­”è®°å½•"""
        if not qa_list:
            return "ï¼ˆè¿™æ˜¯ç¬¬ä¸€ä¸ªé—®é¢˜ï¼‰"
        
        formatted = ""
        for i, qa in enumerate(qa_list, 1):
            formatted += f"\nQ{i}: {qa.get('question', '')}\n"
            formatted += f"A{i}: {qa.get('answer', '')}\n"
            formatted += f"è¯„åˆ†: {qa.get('score', 'N/A')}/10\n"
        return formatted
    
    def _format_rag_results(self, results: List[Dict]) -> str:
        """æ ¼å¼åŒ– RAG æ£€ç´¢ç»“æœ"""
        if not results:
            return "æ— ç›¸å…³é¢˜ç›®"
        
        formatted = ""
        for i, r in enumerate(results, 1):
            formatted += f"\n{i}. [{r.get('category', 'N/A')}] {r['question']}\n"
            formatted += f"   å‚è€ƒ: {r.get('reference_answer', 'N/A')[:100]}...\n"
            formatted += f"   éš¾åº¦: {r.get('difficulty', 3)}/5, ç›¸ä¼¼åº¦: {r.get('score', 0):.2f}\n"
        return formatted
    
    def _format_asked_questions(self, history: List[Dict]) -> str:
        """æ ¼å¼åŒ–å·²é—®è¿‡çš„é—®é¢˜åˆ—è¡¨"""
        if not history:
            return "ï¼ˆæš‚æ— ï¼‰"
        return "\n".join([f"- {q.get('question', '')}" for q in history])
    
    # ==================== å…¬å¼€æ¥å£ ====================
    
    async def generate_question(self, state: InterviewState) -> Dict[str, Any]:
        """
        ç”Ÿæˆé¢è¯•é—®é¢˜ï¼ˆä¸»å…¥å£ï¼‰
        
        Args:
            state: å½“å‰é¢è¯•çŠ¶æ€
            
        Returns:
            åŒ…å«é—®é¢˜å’Œå‚è€ƒç­”æ¡ˆçš„å­—å…¸
        """
        # è¿è¡Œå·¥ä½œæµ
        result = await self.compiled_graph.ainvoke(state)
        
        return {
            "question": result.get('output_question', ''),
            "reference_answer": result.get('output_reference', ''),
            "thinking_message": result.get('thinking_message')
        }


# å•ä¾‹å®ä¾‹
interview_graph = InterviewGraph()
