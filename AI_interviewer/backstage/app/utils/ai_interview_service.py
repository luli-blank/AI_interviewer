"""
AIé¢è¯•æœåŠ¡æ¨¡å—
æä¾›TTSã€ASRã€LLMç­‰AIèƒ½åŠ›çš„å°è£…
"""

import json
import asyncio
from typing import AsyncGenerator, Optional, List, Dict, Any
from openai import AsyncOpenAI
from app.core.config import settings
from dotenv import load_dotenv
import os
import dashscope  # å¼•å…¥ dashscope åŸç”Ÿ SDK
import base64     # å¼•å…¥ base64
import numpy as np # å¼•å…¥ numpy
import struct     # å¼•å…¥ struct ç”¨äºæ„å»º wav å¤´

load_dotenv()
# ==================== é…ç½® ====================
# DeepSeek Chat API é…ç½®
DEEPSEEK_BASE_URL = os.getenv("DEEPSEEK_BASE_URL")

# é˜¿é‡Œäº‘ç™¾ç‚¼ API é…ç½® (ç”¨äº TTS å’Œ ASR)
DASHSCOPE_BASE_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1" # OpenAIå…¼å®¹æ¥å£åœ°å€
DASHSCOPE_API_KEY = os.getenv("DASHSCOPE_API_KEY")

# è®¾ç½® dashscope å…¨å±€é…ç½® (åŸç”Ÿ SDK éœ€è¦)
dashscope.api_key = DASHSCOPE_API_KEY
dashscope.base_http_api_url = 'https://dashscope.aliyuncs.com/api/v1' # åŸç”Ÿ SDK æ¥å£åœ°å€

# ==================== é¢˜åº“æ¨¡æ¿ï¼ˆä¸´æ—¶ï¼Œåç»­è¿ç§»åˆ°æ•°æ®åº“ï¼‰====================
INTERVIEW_QUESTION_BANK = {
    "é€šç”¨é—®é¢˜": [
        {
            "question": "è¯·å…ˆåšä¸€ä¸ªç®€çŸ­çš„è‡ªæˆ‘ä»‹ç»ã€‚",
            "reference_answer": "è§‚å¯Ÿå€™é€‰äººçš„è¡¨è¾¾èƒ½åŠ›ã€é€»è¾‘æ€§ã€ä»¥åŠæ˜¯å¦èƒ½çªå‡ºè‡ªå·±çš„äº®ç‚¹ã€‚",
            "category": "å¼€åœº",
            "difficulty": 1
        },
        {
            "question": "ä½ ä¸ºä»€ä¹ˆæƒ³è¦åº”è˜è¿™ä¸ªå²—ä½ï¼Ÿ",
            "reference_answer": "è€ƒå¯Ÿå€™é€‰äººå¯¹å²—ä½çš„ç†è§£ã€èŒä¸šè§„åˆ’ã€ä»¥åŠåŠ¨æœºæ˜¯å¦åŒ¹é…ã€‚",
            "category": "åŠ¨æœº",
            "difficulty": 1
        },
        {
            "question": "ä½ è§‰å¾—è‡ªå·±æœ€å¤§çš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ",
            "reference_answer": "è€ƒå¯Ÿå€™é€‰äººçš„è‡ªæˆ‘è®¤çŸ¥èƒ½åŠ›ï¼Œä¼˜åŠ¿æ˜¯å¦ä¸å²—ä½è¦æ±‚åŒ¹é…ã€‚",
            "category": "è‡ªæˆ‘è®¤çŸ¥",
            "difficulty": 1
        },
        {
            "question": "ä½ åœ¨å›¢é˜Ÿåˆä½œä¸­é€šå¸¸æ‰®æ¼”ä»€ä¹ˆè§’è‰²ï¼Ÿ",
            "reference_answer": "è€ƒå¯Ÿå›¢é˜Ÿåä½œèƒ½åŠ›ã€è§’è‰²å®šä½ã€ä»¥åŠæ²Ÿé€šèƒ½åŠ›ã€‚",
            "category": "å›¢é˜Ÿåˆä½œ",
            "difficulty": 2
        },
        {
            "question": "è¯·æè¿°ä¸€ä¸ªä½ é‡åˆ°çš„æœ€å¤§æŒ‘æˆ˜ï¼Œä»¥åŠä½ æ˜¯å¦‚ä½•è§£å†³çš„ï¼Ÿ",
            "reference_answer": "è€ƒå¯Ÿé—®é¢˜è§£å†³èƒ½åŠ›ã€æŠ—å‹èƒ½åŠ›ã€ä»¥åŠå¤ç›˜æ€»ç»“èƒ½åŠ›ã€‚",
            "category": "é—®é¢˜è§£å†³",
            "difficulty": 2
        }
    ],
    "æŠ€æœ¯å²—ä½": [
        {
            "question": "è¯·ä»‹ç»ä¸€ä¸ªä½ åšè¿‡çš„æŠ€æœ¯é¡¹ç›®ï¼Œä½ åœ¨å…¶ä¸­è´Ÿè´£ä»€ä¹ˆï¼Ÿ",
            "reference_answer": "è€ƒå¯ŸæŠ€æœ¯æ·±åº¦ã€é¡¹ç›®ç»éªŒã€ä»¥åŠè§’è‰²å®šä½ã€‚",
            "category": "é¡¹ç›®ç»éªŒ",
            "difficulty": 2
        },
        {
            "question": "ä½ å¹³æ—¶æ˜¯å¦‚ä½•å­¦ä¹ æ–°æŠ€æœ¯çš„ï¼Ÿ",
            "reference_answer": "è€ƒå¯Ÿå­¦ä¹ èƒ½åŠ›ã€æŠ€æœ¯çƒ­æƒ…ã€ä»¥åŠæˆé•¿æ½œåŠ›ã€‚",
            "category": "å­¦ä¹ èƒ½åŠ›",
            "difficulty": 1
        }
    ],
    "äº§å“å²—ä½": [
        {
            "question": "ä½ å¦‚ä½•ç†è§£äº§å“ç»ç†è¿™ä¸ªå²—ä½ï¼Ÿ",
            "reference_answer": "è€ƒå¯Ÿå¯¹äº§å“å²—ä½çš„è®¤çŸ¥æ·±åº¦ã€‚",
            "category": "å²—ä½è®¤çŸ¥",
            "difficulty": 1
        },
        {
            "question": "å¦‚æœå¼€å‘è¯´ä½ çš„éœ€æ±‚æ— æ³•å®ç°ï¼Œä½ ä¼šæ€ä¹ˆå¤„ç†ï¼Ÿ",
            "reference_answer": "è€ƒå¯Ÿæ²Ÿé€šåè°ƒèƒ½åŠ›ã€éœ€æ±‚ä¼˜å…ˆçº§åˆ¤æ–­èƒ½åŠ›ã€‚",
            "category": "æ²Ÿé€šåè°ƒ",
            "difficulty": 2
        }
    ],
    "ç»“æŸè¯­": [
        {
            "question": "ä½ è¿˜æœ‰ä»€ä¹ˆæƒ³é—®æˆ‘çš„å—ï¼Ÿ",
            "reference_answer": "é¢è¯•ç»“æŸçš„æ ‡å‡†é—®é¢˜ï¼Œè§‚å¯Ÿå€™é€‰äººçš„æ€è€ƒæ·±åº¦ã€‚",
            "category": "ç»“æŸ",
            "difficulty": 1
        }
    ]
}


class AIInterviewService:
    """AIé¢è¯•æœåŠ¡ç±»"""
    
    def __init__(self):
        # DeepSeek å®¢æˆ·ç«¯ (ç”¨äºæ™ºèƒ½å†³ç­–)
        self.deepseek_client = AsyncOpenAI(
            api_key=os.getenv("Deepseek_API_Key"),
            base_url=DEEPSEEK_BASE_URL
        )
        
        # é˜¿é‡Œäº‘ç™¾ç‚¼å®¢æˆ·ç«¯ (ç”¨äº TTS å’Œ ASR)
        self.dashscope_client = AsyncOpenAI(
            api_key=os.getenv("DASHSCOPE_API_KEY"),
            base_url=DASHSCOPE_BASE_URL
        )
    
    def _add_wav_header(self, pcm_data: bytes, sample_rate: int = 24000, channels: int = 1, bit_depth: int = 16) -> bytes:
        """
        ä¸º PCM æ•°æ®æ·»åŠ  WAV å¤´
        """
        header = bytearray()
        # RIFF chunk
        header.extend(b'RIFF')
        header.extend(struct.pack('<I', 36 + len(pcm_data)))
        header.extend(b'WAVE')
        # fmt chunk
        header.extend(b'fmt ')
        header.extend(struct.pack('<I', 16))  # chunk size
        header.extend(struct.pack('<H', 1))   # format tag (1=PCM)
        header.extend(struct.pack('<H', channels))
        header.extend(struct.pack('<I', sample_rate))
        header.extend(struct.pack('<I', sample_rate * channels * bit_depth // 8)) # byte rate
        header.extend(struct.pack('<H', channels * bit_depth // 8)) # block align
        header.extend(struct.pack('<H', bit_depth))
        # data chunk
        header.extend(b'data')
        header.extend(struct.pack('<I', len(pcm_data)))
        
        return bytes(header) + pcm_data

    # ==================== TTS è¯­éŸ³åˆæˆ ====================
    async def text_to_speech_stream(self, text: str) -> AsyncGenerator[bytes, None]:
        """
        æµå¼æ–‡å­—è½¬è¯­éŸ³ (ä½¿ç”¨ qwen-tts åŸç”Ÿ SDK)
        
        Args:
            text: è¦è½¬æ¢çš„æ–‡æœ¬
            
        Yields:
            éŸ³é¢‘æ•°æ®å— (bytes)
        """
        try:
            # ä¾ç…§å®˜æ–¹ç¤ºä¾‹ä½¿ç”¨ DashScope SDK
            response = dashscope.MultiModalConversation.call(
                api_key=os.getenv("DASHSCOPE_API_KEY"),
                model="qwen3-tts-flash",
                text=text,
                voice="Cherry",
                language_type="Chinese",
                stream=True
            )
            
            for chunk in response:
                if chunk.output is not None and chunk.output.audio is not None:
                    audio = chunk.output.audio
                    if audio.data is not None:
                        # å®˜æ–¹ç¤ºä¾‹è¿”å›çš„æ˜¯ base64 ç¼–ç çš„ WAV/PCM æ•°æ®
                        wav_bytes = base64.b64decode(audio.data)
                        yield wav_bytes
                        
        except Exception as e:
            print(f"TTS Stream Error: {e}")
            yield b""
    
    async def text_to_speech(self, text: str) -> bytes:
        """
        éæµå¼æ–‡å­—è½¬è¯­éŸ³ (ä½¿ç”¨ qwen3-tts-flash åŸç”Ÿ SDK)
        
        Args:
            text: è¦è½¬æ¢çš„æ–‡æœ¬
            
        Returns:
            å®Œæ•´çš„éŸ³é¢‘æ•°æ® (bytes)
        """
        print(f"\n[AI Service] ğŸ¤ Calling TTS (Text-to-Speech)...")
        print(f"[AI Service] Model: qwen3-tts-flash (DashScope SDK)")
        print(f"[AI Service] Input Text: {text[:50]}... (Length: {len(text)})")

        try:
            # å°è£…ä¸€ä¸ªåŒæ­¥è°ƒç”¨å‡½æ•°
            def _sync_call():
                # æ ¹æ®å®˜æ–¹ç¤ºä¾‹ï¼Œå¿…é¡»å¯ç”¨ stream=True æ‰èƒ½è·å–éŸ³é¢‘æµ
                # å³ä½¿æˆ‘ä»¬è¦éæµå¼ç»“æœï¼Œä¹Ÿéœ€è¦æŠŠæµè¯»å®Œæ‹¼èµ·æ¥
                responses = dashscope.MultiModalConversation.call(
                    api_key=os.getenv("DASHSCOPE_API_KEY"),
                    model="qwen3-tts-flash",
                    text=text,
                    voice="Cherry",
                    language_type="Chinese",
                    stream=True
                )
                
                full_audio = bytearray()
                for chunk in responses:
                    if chunk.output is not None and chunk.output.audio is not None:
                        audio = chunk.output.audio
                        if audio.data is not None:
                            wav_bytes = base64.b64decode(audio.data)
                            full_audio.extend(wav_bytes)
                return bytes(full_audio)

            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œ
            loop = asyncio.get_running_loop()
            pcm_content = await loop.run_in_executor(None, _sync_call)
            
            # æ·»åŠ  WAV å¤´
            wav_content = self._add_wav_header(pcm_content)
            
            print(f"[AI Service] âœ… TTS Success. Output Audio Size: {len(wav_content)} bytes")
            return wav_content
            
        except Exception as e:
            print(f"[AI Service] âŒ TTS Error: {e}")
            return b""
    
    # ==================== ASR è¯­éŸ³è¯†åˆ« ====================
    async def speech_to_text_stream(
        self, 
        audio_data: bytes,
        sample_rate: int = 16000
    ) -> AsyncGenerator[str, None]:
        """
        æµå¼è¯­éŸ³è½¬æ–‡å­— (ä½¿ç”¨åŸç”ŸDashScope SDK)
        """
        print(f"\n[AI Service] ğŸ‘‚ Calling ASR Stream...")
        print(f"[AI Service] Input Audio Size: {len(audio_data)} bytes")

        try:
            # ä½¿ç”¨åŸç”Ÿ DashScope SDK è°ƒç”¨ ASR
            import base64
            audio_base64 = base64.b64encode(audio_data).decode('utf-8')
            
            # æ„å»ºæ¶ˆæ¯æ ¼å¼
            messages = [
                {
                    "role": "user",
                    "content": [
                        {
                            "audio": f"data:audio/wav;base64,{audio_base64}"
                        }
                    ]
                }
            ]
            
            # å°è£…ä¸€ä¸ªåŒæ­¥è°ƒç”¨å‡½æ•°
            def _sync_asr_call():
                response = dashscope.MultiModalConversation.call(
                    api_key=os.getenv("DASHSCOPE_API_KEY"),
                    model="qwen3-asr-flash",
                    messages=messages,
                    result_format="message",
                    asr_options={
                        "enable_itn": False  # ä¸å¯ç”¨é€†æ–‡æœ¬è§„èŒƒåŒ–
                    }
                )
                return response
            
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥è°ƒç”¨
            loop = asyncio.get_running_loop()
            response = await loop.run_in_executor(None, _sync_asr_call)
            
            # è§£æå“åº”
            if response.status_code == 200:
                result_text = response.output.choices[0].message.content
                print(f"[AI Service] âœ… ASR Success. Result: {result_text}")
                yield result_text
            else:
                print(f"[AI Service] âŒ ASR Error: {response}")
                yield ""
                    
        except Exception as e:
            print(f"[AI Service] âŒ ASR Error: {e}")
            yield ""
    
    async def speech_to_text(self, audio_data: bytes) -> str:
        """
        éæµå¼è¯­éŸ³è½¬æ–‡å­— (ä½¿ç”¨åŸç”ŸDashScope SDK)
        """
        print(f"\n[AI Service] ğŸ‘‚ Calling ASR (Speech-to-Text)...")
        print(f"[AI Service] Input Audio Size: {len(audio_data)} bytes")
        
        try:
            # ä½¿ç”¨åŸç”Ÿ DashScope SDK è°ƒç”¨ ASR
            import base64
            audio_base64 = base64.b64encode(audio_data).decode('utf-8')
            
            # æ„å»ºæ¶ˆæ¯æ ¼å¼
            messages = [
                {
                    "role": "user",
                    "content": [
                        {
                            "audio": f"data:audio/wav;base64,{audio_base64}"
                        }
                    ]
                }
            ]
            
            # å°è£…ä¸€ä¸ªåŒæ­¥è°ƒç”¨å‡½æ•°
            def _sync_asr_call():
                response = dashscope.MultiModalConversation.call(
                    api_key=os.getenv("DASHSCOPE_API_KEY"),
                    model="qwen3-asr-flash",
                    messages=messages,
                    result_format="message",
                    asr_options={
                        "enable_itn": False  # ä¸å¯ç”¨é€†æ–‡æœ¬è§„èŒƒåŒ–
                    }
                )
                return response
            
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥è°ƒç”¨
            loop = asyncio.get_running_loop()
            response = await loop.run_in_executor(None, _sync_asr_call)
            
            # è§£æå“åº”
            if response.status_code == 200:
                content = response.output.choices[0].message.content
                
                # å¤„ç†è¿”å›ç»“æœï¼šå¯èƒ½æ˜¯å­—ç¬¦ä¸²ã€åˆ—è¡¨æˆ–å­—å…¸
                if isinstance(content, str):
                    # å°è¯•è§£æJSONå­—ç¬¦ä¸²
                    try:
                        import json
                        parsed = json.loads(content)
                        if isinstance(parsed, list) and len(parsed) > 0:
                            # æå–åˆ—è¡¨ä¸­ç¬¬ä¸€ä¸ªå…ƒç´ çš„textå­—æ®µ
                            if isinstance(parsed[0], dict) and 'text' in parsed[0]:
                                final_text = parsed[0]['text']
                            else:
                                final_text = str(parsed[0])
                        else:
                            final_text = content
                    except (json.JSONDecodeError, ValueError):
                        # å¦‚æœä¸æ˜¯JSONï¼Œç›´æ¥ä½¿ç”¨åŸå­—ç¬¦ä¸²
                        final_text = content
                elif isinstance(content, list) and len(content) > 0:
                    # å¦‚æœå·²ç»æ˜¯åˆ—è¡¨ï¼Œæå–textå­—æ®µ
                    if isinstance(content[0], dict) and 'text' in content[0]:
                        final_text = content[0]['text']
                    else:
                        final_text = str(content[0])
                elif isinstance(content, dict) and 'text' in content:
                    final_text = content['text']
                else:
                    final_text = str(content)
                
                print(f"[AI Service] âœ… ASR Final Result: '{final_text}'")
                return final_text.strip() if isinstance(final_text, str) else str(final_text)
            else:
                print(f"[AI Service] âŒ ASR Error: {response}")
                return ""
                
        except Exception as e:
            print(f"[AI Service] âŒ ASR Error: {e}")
            return ""
    
    # ==================== LLM æ™ºèƒ½å†³ç­– ====================
    async def generate_interview_questions(
        self,
        resume_text: str,
        job_name: str,
        num_questions: int = 8
    ) -> List[Dict[str, Any]]:
        """
        æ ¹æ®ç®€å†å’Œå²—ä½ç”Ÿæˆé¢è¯•é—®é¢˜
        
        Args:
            resume_text: ç®€å†æ–‡æœ¬
            job_name: ç›®æ ‡å²—ä½åç§°
            num_questions: éœ€è¦ç”Ÿæˆçš„é—®é¢˜æ•°é‡
            
        Returns:
            é—®é¢˜åˆ—è¡¨ï¼Œæ¯ä¸ªé—®é¢˜åŒ…å« question, reference_answer, category
        """
        print(f"\n[AI Service] ğŸ§  Calling LLM (Generate Questions)...")
        print(f"[AI Service] Model: deepseek-chat")
        print(f"[AI Service] Job: {job_name}, Generating {num_questions} questions")

        all_questions = []
        all_questions.extend(INTERVIEW_QUESTION_BANK["é€šç”¨é—®é¢˜"])
        
        # æ ¹æ®å²—ä½ç±»å‹æ·»åŠ ä¸“ä¸šé—®é¢˜
        if "æŠ€æœ¯" in job_name or "å¼€å‘" in job_name or "å·¥ç¨‹å¸ˆ" in job_name:
            all_questions.extend(INTERVIEW_QUESTION_BANK["æŠ€æœ¯å²—ä½"])
        elif "äº§å“" in job_name:
            all_questions.extend(INTERVIEW_QUESTION_BANK["äº§å“å²—ä½"])
        
        prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„é¢è¯•å®˜ï¼Œè¯·æ ¹æ®å€™é€‰äººçš„ç®€å†å’Œç›®æ ‡å²—ä½ï¼Œä»ä»¥ä¸‹é¢˜åº“ä¸­é€‰æ‹©æœ€åˆé€‚çš„{num_questions}ä¸ªé—®é¢˜ï¼Œå¹¶å¯ä»¥æ ¹æ®ç®€å†å†…å®¹å¯¹é—®é¢˜è¿›è¡Œä¸ªæ€§åŒ–è°ƒæ•´ã€‚

## å€™é€‰äººç®€å†ï¼š
{resume_text[:3000]}  # é™åˆ¶é•¿åº¦é˜²æ­¢tokenæº¢å‡º

## ç›®æ ‡å²—ä½ï¼š{job_name}

## å¯é€‰é¢˜åº“ï¼š
{json.dumps(all_questions, ensure_ascii=False, indent=2)}

## è¦æ±‚ï¼š
1. é€‰æ‹©ä¸å€™é€‰äººèƒŒæ™¯å’Œç›®æ ‡å²—ä½æœ€åŒ¹é…çš„é—®é¢˜
2. å¯ä»¥æ ¹æ®ç®€å†ä¸­çš„å…·ä½“å†…å®¹è°ƒæ•´é—®é¢˜ï¼Œä½¿å…¶æ›´æœ‰é’ˆå¯¹æ€§
3. é—®é¢˜éš¾åº¦åº”è¯¥å¾ªåºæ¸è¿›ï¼Œä»ç®€å•åˆ°å¤æ‚
4. æœ€åä¸€ä¸ªé—®é¢˜åº”è¯¥æ˜¯ç»“æŸè¯­

è¯·ä»¥JSONæ•°ç»„æ ¼å¼è¿”å›ï¼Œæ¯ä¸ªé—®é¢˜åŒ…å«ï¼š
- question: é—®é¢˜å†…å®¹
- reference_answer: å‚è€ƒè¯„åˆ¤æ ‡å‡†
- category: é—®é¢˜ç±»åˆ«
- is_from_resume: æ˜¯å¦åŸºäºç®€å†å†…å®¹å®šåˆ¶ (true/false)

åªè¿”å›JSONæ•°ç»„ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
"""
        
        try:
            response = await self.deepseek_client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„AIé¢è¯•å®˜åŠ©æ‰‹ï¼Œæ“…é•¿æ ¹æ®ç®€å†å®šåˆ¶é¢è¯•é—®é¢˜ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                response_format={"type": "json_object"}
            )
            
            result = response.choices[0].message.content
            # è§£æJSON
            questions = json.loads(result)
            if isinstance(questions, dict) and "questions" in questions:
                questions = questions["questions"]
            
            final_questions = questions[:num_questions]
            print(f"[AI Service] âœ… Questions Generated: {len(final_questions)}")
            return final_questions
            
        except Exception as e:
            print(f"[AI Service] âŒ Question generation error: {e}")
            print(f"Question generation error: {e}")
            # è¿”å›é»˜è®¤é—®é¢˜
            return INTERVIEW_QUESTION_BANK["é€šç”¨é—®é¢˜"][:num_questions]
    
    async def analyze_answer_and_decide(
        self,
        current_question: str,
        reference_answer: str,
        user_answer: str,
        resume_text: str,
        question_history: List[Dict],
        remaining_questions: int
    ) -> Dict[str, Any]:
        """
        åˆ†æç”¨æˆ·å›ç­”å¹¶å†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨
        
        Args:
            current_question: å½“å‰é—®é¢˜
            reference_answer: å‚è€ƒç­”æ¡ˆ/è¯„åˆ¤æ ‡å‡†
            user_answer: ç”¨æˆ·çš„å›ç­”
            resume_text: ç®€å†æ–‡æœ¬
            question_history: å·²å®Œæˆçš„é—®ç­”å†å²
            remaining_questions: å‰©ä½™é—®é¢˜æ•°é‡
            
        Returns:
            å†³ç­–ç»“æœï¼ŒåŒ…å«ï¼š
            - action: "follow_up" | "next_question" | "end_interview"
            - follow_up_question: è¿½é—®é—®é¢˜ï¼ˆå¦‚æœactionæ˜¯follow_upï¼‰
            - score: æœ¬é¢˜è¯„åˆ† (1-10)
            - feedback: ç®€çŸ­è¯„ä»·
            - reason: å†³ç­–ç†ç”±
        """
        history_text = "\n".join([
            f"Q: {h['question']}\nA: {h['answer']}\nè¯„åˆ†: {h.get('score', 'N/A')}"
            for h in question_history[-3:]  # åªå–æœ€è¿‘3è½®
        ])
        
        prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±é¢è¯•å®˜ï¼Œè¯·åˆ†æå€™é€‰äººçš„å›ç­”å¹¶å†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨ã€‚

## å½“å‰é—®é¢˜ï¼š
{current_question}

## å‚è€ƒè¯„åˆ¤æ ‡å‡†ï¼š
{reference_answer}

## å€™é€‰äººå›ç­”ï¼š
{user_answer}

## å€™é€‰äººç®€å†æ‘˜è¦ï¼š
{resume_text[:1500]}

## æœ€è¿‘é—®ç­”å†å²ï¼š
{history_text if history_text else "ï¼ˆè¿™æ˜¯ç¬¬ä¸€ä¸ªé—®é¢˜ï¼‰"}

## å‰©ä½™é—®é¢˜æ•°ï¼š{remaining_questions}

## è¯·åˆ†æå¹¶å†³ç­–ï¼š

1. è¯„ä¼°å›ç­”è´¨é‡ï¼ˆ1-10åˆ†ï¼‰ï¼š
   - å®Œæ•´æ€§ï¼šæ˜¯å¦æ­£é¢å›ç­”äº†é—®é¢˜
   - æ·±åº¦ï¼šæ˜¯å¦æœ‰å…·ä½“æ¡ˆä¾‹å’Œæ•°æ®
   - é€»è¾‘æ€§ï¼šè¡¨è¾¾æ˜¯å¦æ¸…æ™°è¿è´¯
   - çœŸå®æ€§ï¼šä¸ç®€å†æè¿°æ˜¯å¦ä¸€è‡´

2. å†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼š
   - "follow_up": éœ€è¦è¿½é—®ï¼ˆå›ç­”ä¸å®Œæ•´ã€æœ‰ç–‘ç‚¹ã€å‘ç°äº®ç‚¹éœ€è¦æ·±æŒ–ï¼‰
   - "next_question": è¿›å…¥ä¸‹ä¸€é¢˜ï¼ˆå›ç­”å……åˆ†æˆ–è¿½é—®å·²è¾¾2æ¬¡ï¼‰
   - "end_interview": ç»“æŸé¢è¯•ï¼ˆæ‰€æœ‰æ ¸å¿ƒé—®é¢˜å·²é—®å®Œæˆ–æ—¶é—´è¶…è¿‡25åˆ†é’Ÿï¼‰

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
    "score": è¯„åˆ†æ•°å­—,
    "feedback": "ç®€çŸ­è¯„ä»·ï¼Œä¸è¶…è¿‡50å­—",
    "action": "follow_up/next_question/end_interview",
    "follow_up_question": "è¿½é—®é—®é¢˜ï¼ˆä»…å½“actionä¸ºfollow_upæ—¶éœ€è¦ï¼‰",
    "reason": "å†³ç­–ç†ç”±ï¼Œä¸è¶…è¿‡30å­—"
}}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
"""
        
        try:
            print(f"\n[AI Service] âš–ï¸ Calling LLM (Analyze Answer)...")
            print(f"[AI Service] Model: deepseek-chat")
            
            response = await self.deepseek_client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„AIé¢è¯•å®˜ï¼Œæ“…é•¿è¯„ä¼°å€™é€‰äººçš„å›ç­”å¹¶åšå‡ºåˆç†å†³ç­–ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            print(f"[AI Service] âœ… Analysis Result: Action={result.get('action')}, Score={result.get('score')}")
            return result
            
        except Exception as e:
            print(f"[AI Service] âŒ Answer analysis error: {e}")
            # é»˜è®¤è¿›å…¥ä¸‹ä¸€é¢˜
            return {
                "score": 5,
                "feedback": "ç³»ç»Ÿå¤„ç†ä¸­",
                "action": "next_question",
                "reason": "ç³»ç»Ÿé»˜è®¤"
            }
    
    async def generate_interview_opening(self, candidate_name: str = "åŒå­¦") -> str:
        """ç”Ÿæˆé¢è¯•å¼€åœºç™½"""
        return f"ä½ å¥½{candidate_name}ï¼Œæˆ‘æ˜¯ä»Šå¤©çš„AIé¢è¯•å®˜ã€‚åœ¨å¼€å§‹æ­£å¼é¢è¯•ä¹‹å‰ï¼Œè¯·ç¡®è®¤ä½ çš„æ‘„åƒå¤´å’Œéº¦å…‹é£å·²ç»å‡†å¤‡å°±ç»ªã€‚å‡†å¤‡å¥½äº†å—ï¼Ÿå‡†å¤‡å¥½å°±å¯ä»¥å¼€å§‹äº†ã€‚"
    
    async def generate_interview_closing(
        self,
        question_history: List[Dict],
        overall_score: float
    ) -> str:
        """ç”Ÿæˆé¢è¯•ç»“æŸè¯­"""
        prompt = f"""è¯·æ ¹æ®é¢è¯•æƒ…å†µç”Ÿæˆä¸€æ®µä¸“ä¸šã€æ¸©å’Œçš„ç»“æŸè¯­ã€‚

## é¢è¯•é—®ç­”å†å²ï¼š
{json.dumps(question_history[-5:], ensure_ascii=False)}

## æ•´ä½“è¯„åˆ†ï¼š{overall_score:.1f}/10

è¦æ±‚ï¼š
1. æ„Ÿè°¢å€™é€‰äººçš„å‚ä¸
2. ç®€è¦è‚¯å®šè¡¨ç°äº®ç‚¹ï¼ˆå¦‚æœæœ‰ï¼‰
3. è¯´æ˜åç»­æµç¨‹
4. è¯­æ°”ä¸“ä¸šå‹å–„
5. æ§åˆ¶åœ¨100å­—ä»¥å†…

åªè¾“å‡ºç»“æŸè¯­å†…å®¹ï¼Œä¸è¦å…¶ä»–æ ¼å¼ã€‚
"""
        
        try:
            response = await self.deepseek_client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=200
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            print(f"Closing generation error: {e}")
            return "å¥½çš„ï¼Œä»Šå¤©çš„é¢è¯•å°±åˆ°è¿™é‡Œã€‚æ„Ÿè°¢ä½ çš„å‚ä¸ï¼Œåç»­ç»“æœæˆ‘ä»¬ä¼šé€šè¿‡é‚®ä»¶é€šçŸ¥ä½ ã€‚ç¥ä½ ä¸€åˆ‡é¡ºåˆ©ï¼"
    
    async def stream_text_response(
        self,
        prompt: str,
        system_prompt: str = "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„AIé¢è¯•å®˜ã€‚"
    ) -> AsyncGenerator[str, None]:
        """
        æµå¼ç”Ÿæˆæ–‡æœ¬å“åº”
        
        Args:
            prompt: ç”¨æˆ·æç¤º
            system_prompt: ç³»ç»Ÿæç¤º
            
        Yields:
            æ–‡æœ¬ç‰‡æ®µ
        """
        try:
            response = await self.deepseek_client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                stream=True
            )
            
            async for chunk in response:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
                    
        except Exception as e:
            print(f"Stream text error: {e}")
            yield "æŠ±æ­‰ï¼Œç³»ç»Ÿå¤„ç†ä¸­ï¼Œè¯·ç¨å€™..."


# å•ä¾‹å®ä¾‹
ai_interview_service = AIInterviewService()
